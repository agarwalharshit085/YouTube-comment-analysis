# -*- coding: utf-8 -*-
"""YouTube Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14X2cDzASoPPw2k9EBvA1-e6oQzCGB8Sr
"""

!pip install datasets

import pandas as pd
from sklearn.model_selection import train_test_split
from datasets import Dataset, DatasetDict
from transformers import AutoTokenizer, TrainingArguments, Trainer
from transformers import AutoModelForSequenceClassification as SeqModClf
import torch
import numpy
from sklearn.metrics import accuracy_score, f1_score

from google.colab import files
uploaded = files.upload()

import pandas as pd

dataset = pd.read_csv("YoutubeCommentsDataSet.csv")
dataset.head()

dataset.info()

dataset.isnull().sum()

dataset.dropna()

#encode train & test
def encode_labels(dataset):
     dataset['Sentiment'] = dataset['Sentiment'].replace({'negative':0,'neutral':1,'positive':2})
     return dataset

encoded_dataset = encode_labels(dataset)

encoded_dataset.head()

encoded_dataset = encoded_dataset.rename(columns={'Comment': 'text', 'Sentiment': 'label'})

#fix class imbalance with train-test split
train, test = train_test_split(encoded_dataset, test_size = 0.3,
                               random_state = 42,
                               stratify = encoded_dataset['label'])

train.to_csv("train.csv", index=True)
test.to_csv("test.csv", index=True)

train_hf = Dataset.from_pandas(train.reset_index(drop= True))
test_hf = Dataset.from_pandas(test.reset_index(drop= True))

print(train_hf[:1])

print(test_hf[:1])

datasets = DatasetDict({ 'train': train_hf, 'test' : test_hf})

tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

def tokenizer_function(dataset):
    comments = [str(comment) for comment in dataset['text']]
    return tokenizer(comments, padding = 'max_length',truncation = True)

tokenized_datasets = datasets.map(tokenizer_function, batched= True)

tokenized_datasets = tokenized_datasets.remove_columns(['text'])
tokenized_datasets.set_format('tf')

print(tokenized_datasets['train'][0])

model = SeqModClf.from_pretrained('distilbert-base-uncased', num_labels = 3)

device  =  torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)
print(f'model moved to {device}')

# !pip install --upgrade transformers
!pip install torch

# !pip install --upgrade transformers # Make sure transformers is updated
from transformers import TrainingArguments, Trainer # Import the Trainer class
import torch
training_args = TrainingArguments(
    output_dir='/kaggle/working/',
    num_train_epochs=0.01,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    warmup_steps=50,
    weight_decay=0.01,
    # evaluation_strategy="epoch", # This argument is likely causing the issue in older versions. Remove or comment it out
    # save_strategy="epoch", # If you still encounter issues, try commenting this out as well
    # load_best_model_at_end=True, # And this one too
    # metric_for_best_model="eval_f1_score",
    # greater_is_better = True,
    # report_to="none",
    fp16=torch.cuda.is_available(),
)

def compute_metrics(pred):
    logits, labels = pred
    predictions = numpy.argmax(logits, axis = -1)
    Accuracy_score = accuracy_score(labels,predictions)
    F1_score = f1_score(labels, predictions, average = 'weighted')
    return {'accuracy_score':Accuracy_score,'f1_score':F1_score}

trainer = Trainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_datasets['train'],
    eval_dataset = tokenized_datasets['test'],
    tokenizer = tokenizer,
    compute_metrics = compute_metrics
)

trainer.train()

print(trainer.evaluate())

trainer.save_model('/kaggle/working/distilbert-base-uncased-finetuned-youtube-comment-sentiment')
tokenizer.save_pretrained('/kaggle/working/distilbert-base-uncased-finetuned-youtube-comment-sentiment-tokenizer')

from transformers import pipeline

sentiment_classifier = pipeline(
    'sentiment-analysis',
    model = '/kaggle/working/distilbert-base-uncased-finetuned-youtube-comment-sentiment',
    tokenizer = '/kaggle/working/distilbert-base-uncased-finetuned-youtube-comment-sentiment-tokenizer',
    device = 0 if torch.cuda.is_available() else -1
)

new_comment_1 = "This is an amazing tutorial, very helpful!"
new_comment_2 = "I am quite confused by this explanation."
new_comment_3 = "It's okay, neither good nor bad."

predictions = sentiment_classifier([new_comment_1, new_comment_2, new_comment_3])

for comment, prediction in zip([new_comment_1, new_comment_2, new_comment_3], predictions):
    print(f"Comment: \"{comment}\"")
    predicted_label_str = prediction['label']
    predicted_label_int = int(predicted_label_str.split('_')[-1])
    score = prediction['score']
    print(f"Predicted Sentiment (Label): {predicted_label_int} (Confidence: {score:.4f})")

# prompt: Generate a pkl file for the trained model and attach the yh model

import pickle

# Save the trained model
with open('/kaggle/working/trained_model.pkl', 'wb') as f:
    pickle.dump(trainer.model, f)

# Save the tokenizer
with open('/kaggle/working/tokenizer.pkl', 'wb') as f:
    pickle.dump(tokenizer, f)